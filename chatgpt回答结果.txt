问题1：Maven如何将普通java程序打成jar包直接运行

要将普通的Java程序打成可执行的JAR包，您需要遵循以下步骤来配置Maven：

1. 创建一个Maven项目并在pom.xml文件中添加以下插件依赖项：

```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-jar-plugin</artifactId>
            <version>3.2.0</version>
            <configuration>
                <archive>
                    <manifest>
                        <addClasspath>true</addClasspath>
                        <mainClass>com.example.MainClass</mainClass>
                    </manifest>
                </archive>
            </configuration>
        </plugin>
    </plugins>
</build>
```

上述插件依赖项指定了Maven JAR插件版本、JAR包归档配置，以及主类（即包含main()方法的类）。

2. 将您的源代码和资源文件放置在标准的Maven项目目录结构下，如src/main/java和src/main/resources。

3. 执行以下命令来生成JAR包：

```
mvn clean package
```

该命令将编译您的代码并将其打包为JAR文件。在target目录中，您将找到一个名为`your-project-name.jar`的可执行JAR文件。

4. 运行JAR文件：

```
java -jar your-project-name.jar
```

注意：确保替换“your-project-name”为实际的项目名称，并将“com.example.MainClass”替换为您的主类的完整名称。如果您的主类在包中，则将其完整路径包含在内（例如，“com.example.myapp.MainClass”）。



使用 impdp 命令可以导入 Oracle 数据库中的表。以下是导入单张表的步骤：

1. 打开命令行终端或 SQL*Plus 环境。
2. 使用管理员账户登录到数据库。
3. 输入以下命令来导出源表的元数据和数据：

   ```
   expdp username/password@database_name schemas=schema_name tables=table_name directory=directory_name dumpfile=dump_file_name.dmp logfile=log_file_name.log
   ```
   
   其中，username 是数据库用户名称，password 是密码，database_name 是数据库名称，schema_name 是表所在的模式名称，table_name 是要导出的表名，directory_name 是指定导出文件的目录，dump_file_name.dmp 是导出文件的名称，log_file_name.log 是导出日志文件的名称。如果需要导出多个表，可以在 tables 参数中指定多个表名，用逗号分隔。
   
4. 将导出的文件传输到目标数据库服务器上。
5. 在目标数据库服务器上打开命令行终端或 SQL*Plus 环境。
6. 使用管理员账户登录到数据库。
7. 输入以下命令来导入源表的元数据和数据：

   ```
   impdp username/password@database_name schemas=schema_name tables=table_name directory=directory_name dumpfile=dump_file_name.dmp logfile=log_file_name.log
   ```

   其中，username 是数据库用户名称，password 是密码，database_name 是数据库名称，schema_name 是表所在的模式名称，table_name 是要导入的表名，directory_name 是指定导入文件的目录，dump_file_name.dmp 是导出文件的名称，log_file_name.log 是导入日志文件的名称。如果需要导入多个表，可以在 tables 参数中指定多个表名，用逗号分隔。

8. 等待导入完成。

注意：在导入表之前，请确保目标数据库中不存在同名的表。如果存在同名表，可以使用 impdp 命令的 REMAP_TABLE 参数来重命名要导入的表。

在Redis中保证缓存一致性的常见方法有以下几种：

1. 过期时间随机化

为了防止大量的缓存同时失效而导致瞬间压力过大，可以使用过期时间随机化来实现缓存失效的分散。具体方式是在设置过期时间时，加上一个小的随机值，让不同的缓存失效时间略有不同。

示例代码：

```
import random
import redis

r = redis.Redis(host='localhost', port=6379, db=0)

def set_cache(key, value, expire_time):
    # 随机化过期时间
    expire_time += random.randint(0, 10)
    r.setex(key, value, expire_time)
```

2. 双写/三写

双写或三写策略指的是在更新数据库后，再更新对应的缓存，以保证缓存和数据库的数据一致。其中双写是指将更新操作封装到一个事务中进行，一次性地更新数据库和缓存；三写是指先更新主数据库，再更新从数据库，最后再更新缓存。

示例代码：

```
def update_data_in_database(id, data):
    # 更新数据库
    # ...

    # 更新缓存
    r.set('data_' + id, data)
```

3. 延迟双删

由于缓存中的数据可能会被频繁地更新，如果每次更新都立即删除缓存，这样做反而会导致缓存的命中率下降。延迟双删策略是指在更新操作后，先不删除缓存，而是等到缓存过期时再删除。

示例代码：

```
def update_data_in_database(id, data):
    # 更新数据库
    # ...

    # 延迟双删：等到缓存过期时再删除
    r.set('data_' + id, data)
    r.expire('data_' + id, 60)  # 设置缓存过期时间为60秒
```

4. 布隆过滤器

布隆过滤器是一种数据结构，它可以用于快速判断一个元素是否存在于一个集合中，同时能够有效地减轻缓存和数据库之间的压力。具体方式是在缓存中设置一个布隆过滤器，每次查询时先通过布隆过滤器判断元素是否可能存在于缓存中，如果可能存在，则再去缓存中查找。

示例代码：

```
import redis
from pybloom_live import BloomFilter

r = redis.Redis(host='localhost', port=6379, db=0)

bf = BloomFilter(capacity=1000000, error_rate=0.001)
# 将数据添加到布隆过滤器中
bf.add('data_1')
bf.add('data_2')

def get_data(id):
    if id in bf:
        # 可能存在于缓存中，继续查找
        return r.get('data_' + id)
    else:
        # 不存在于缓存中，从数据库中查找
        # ...
```

需要注意的是，以上方案并不是绝对可靠的，它们都有各自的局限性和适用场景，需要根据具体业务需求和环境特点灵活选择。
布隆过滤器是一种快速、空间效率高的概率数据结构，用于判断一个元素是否属于一个集合。它基于哈希函数和位向量实现。

以下是一个简单的布隆过滤器的Java实现：

```java
import java.util.BitSet;
import java.util.Random;

public class BloomFilter {
    
    private int size; // 位向量的大小
    private BitSet bitSet; // 位向量
    private int hashFunctions; // 哈希函数的个数
    private Random rand; // 随机数生成器
    
    public BloomFilter(int size, int hashFunctions) {
        this.size = size;
        this.bitSet = new BitSet(size);
        this.hashFunctions = hashFunctions;
        this.rand = new Random();
    }
    
    // 添加元素
    public void add(String element) {
        for (int i = 0; i < hashFunctions; i++) {
            int hash = Math.abs(rand.nextInt()) % size;
            bitSet.set(hash, true);
        }
    }
    
    // 判断元素是否存在
    public boolean contains(String element) {
        for (int i = 0; i < hashFunctions; i++) {
            int hash = Math.abs(rand.nextInt()) % size;
            if (!bitSet.get(hash)) {
                return false;
            }
        }
        return true;
    }
}
```

在这个实现中，我们使用了一个BitSet类来表示位向量，并使用Random类来生成随机数作为哈希函数的种子。对于添加元素操作，我们使用了多个哈希函数来计算元素的哈希值，并将对应的位设置为1。对于查找元素操作，我们也使用相同数量的哈希函数来计算元素的哈希值，并检查对应的位是否都被设置为1，如果有一个位没有被设置为1，则元素肯定不存在于集合中。

需要注意的是，布隆过滤器存在一定的误判率。当元素不存在于集合中时，它可能会被错误地认为存在于集合中。因此，布隆过滤器适用于对误判率要求不高的场景，比如缓存、搜索引擎等。


这些都是Druid SQL解析器（SQLParser）中的特性选项，下面是它们的具体作用：

1. KeepInsertValueClauseOriginalString：保留INSERT语句中VALUES子句中原始字符串的信息，方便后续使用。

2. KeepSelectListOriginalString：保留SELECT语句中选择列表中每个列的原始字符串的信息，方便后续使用。

3. UseInsertColumnsCache：启用INSERT语句的列缓存，可提高INSERT语句的执行效率。

4. EnableSQLBinaryOpExprGroup：启用SQL二进制操作表达式分组，以减少括号数量并提高查询性能。

5. OptimizedForParameterized：针对参数化查询进行优化，可以提高查询性能。

6. OptimizedForForParameterizedSkipValue：针对参数化查询跳过某些值进行优化，可以提高查询性能。

7. KeepComments：保留SQL语句中的注释信息。

8. SkipComments：跳过SQL语句中的注释信息。

9. StrictForWall：采用严格的SQL解析策略，以遵循防火墙规则。

10. TDDLHint：支持TDDL分库分表的SQL Hint；

11. DRDSAsyncDDL：DRDS异步DDL支持。

12. DRDSBaseline：DRDS基线支持。

13. InsertReader：启用INSERT语句的READER模式，可以提高INSERT语句的执行效率。

14. IgnoreNameQuotes：忽略SQL语句中的名称引号。

15. KeepNameQuotes：保留SQL语句中的名称引号。

16. SelectItemGenerateAlias：生成选择列表项的别名。

17. PipesAsConcat：将管道符“|”视为字符串连接符进行解析。

18. InsertValueCheckType：检查INSERT语句的值类型。

19. InsertValueNative：在INSERT语句中支持本地化值。

20. EnableCurrentTimeExpr：启用当前时间表达式。

21. EnableCurrentUserExpr：启用当前用户表达式。

22. KeepSourceLocation：保留SQL源代码位置信息。

23. SupportUnicodeCodePoint：支持Unicode码点。

24. PrintSQLWhileParsingFailed：在SQL解析失败时输出SQL语句。

25. EnableMultiUnion：启用多重UNION查询。

26. Spark：当SQL语句使用Spark作为计算引擎时，需要打开此选项。

27. Presto：当SQL语句使用Presto作为计算引擎时，需要打开此选项。